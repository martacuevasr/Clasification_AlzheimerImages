{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irh3NxmfHwZf",
        "outputId": "ca6478c4-0c4b-48ff-c939-0c24a451f6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/yiweilu2033/well-documented-alzheimers-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.02G/4.02G [00:44<00:00, 96.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/yiweilu2033/well-documented-alzheimers-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"yiweilu2033/well-documented-alzheimers-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzVRn0FgHzkQ",
        "outputId": "0a4d4c50-68fd-46fc-9d1d-a06b823ea4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ModerateDemented', 'NonDemented (2)', 'oasis_cross-sectional-5708aa0a98d82080 (1).xlsx', 'VeryMildDemented', 'MildDemented']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def count_images_in_folders(dataset_path):\n",
        "    folder_counts = {}\n",
        "\n",
        "    for folder in os.listdir(dataset_path):\n",
        "        folder_path = os.path.join(dataset_path, folder)\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Buscar subcarpeta con el mismo nombre (excepto para 'NonDemented')\n",
        "            subfolder_name = folder if \"NonDemented\" not in folder else \"NonDemented\"\n",
        "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
        "\n",
        "            if os.path.isdir(subfolder_path):\n",
        "                image_count = len([f for f in os.listdir(subfolder_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))])\n",
        "                folder_counts[folder] = image_count\n",
        "            else:\n",
        "                print(f\"Subcarpeta no encontrada en: {folder_path}\")\n",
        "\n",
        "    return folder_counts\n",
        "\n",
        "# Ruta del dataset\n",
        "image_counts = count_images_in_folders(path)\n",
        "\n",
        "# Imprimir resultados\n",
        "for folder, count in image_counts.items():\n",
        "    print(f\"{folder}: {count} imágenes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClhuBAWuH4La",
        "outputId": "4d02309c-3414-4fab-ab6b-bc0404cd1a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModerateDemented: 376 imágenes\n",
            "NonDemented (2): 63560 imágenes\n",
            "VeryMildDemented: 13796 imágenes\n",
            "MildDemented: 5184 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imgaug\n",
        "!pip install numpy==1.24.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "id": "V76m_rHpRwBT",
        "outputId": "043e6633-e391-4edf-9220-25614967c4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imgaug\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imgaug) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from imgaug) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from imgaug) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.11/dist-packages (from imgaug) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from imgaug) (4.11.0.86)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.37.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.0.7)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (2025.3.13)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (2.8.2)\n",
            "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imgaug\n",
            "Successfully installed imgaug-0.4.0\n",
            "Collecting numpy==1.24.0\n",
            "  Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d3142e96502845dd8451d6f3c11bc8d3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imgaug.augmenters as iaa\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Configurar las transformaciones\n",
        "augmenters = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5),  # Volteo horizontal\n",
        "    iaa.Affine(rotate=(-15, 15)),  # Rotación aleatoria\n",
        "    iaa.GaussianBlur(sigma=(0, 1.0)),  # Desenfoque aleatorio\n",
        "])\n",
        "\n",
        "# Directorio donde guardar imágenes aumentadas\n",
        "augmented_dir = \"/root/.cache/kagglehub/datasets/augmented_dataset\"\n",
        "os.makedirs(augmented_dir, exist_ok=True)\n",
        "\n",
        "# Directorio base donde están las imágenes\n",
        "base_dir = path\n",
        "\n",
        "# Diccionario con la estructura correcta de carpetas y subcarpetas (sin ModerateDemented)\n",
        "category_subfolders = {\n",
        "    \"MildDemented\": \"MildDemented\",\n",
        "    \"VeryMildDemented\": \"VeryMildDemented\",\n",
        "    \"NonDemented (2)\": \"NonDemented\"\n",
        "}\n",
        "\n",
        "# Número de imágenes objetivo por clase (sin ModerateDemented)\n",
        "target_counts = {\n",
        "    \"MildDemented\": 8000,\n",
        "    \"VeryMildDemented\": 15000,\n",
        "    \"NonDemented\": None  # No se aumenta esta categoría\n",
        "}\n",
        "\n",
        "for category, subfolder in category_subfolders.items():\n",
        "    category_path = os.path.join(base_dir, category, subfolder)\n",
        "\n",
        "    if not os.path.exists(category_path) or category not in target_counts:\n",
        "        continue\n",
        "\n",
        "    images = [img for img in os.listdir(category_path) if img.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
        "\n",
        "    if target_counts[category] is None or len(images) >= target_counts[category]:\n",
        "        continue  # No se hace augmentación si ya hay suficientes imágenes\n",
        "\n",
        "    new_category_path = os.path.join(augmented_dir, category)\n",
        "    os.makedirs(new_category_path, exist_ok=True)\n",
        "\n",
        "    augmentation_needed = target_counts[category] - len(images)\n",
        "\n",
        "    for i in range(augmentation_needed):\n",
        "        img_name = images[i % len(images)]  # Seleccionar imágenes repetidamente si es necesario\n",
        "        img_path = os.path.join(category_path, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Aplicar augmentación\n",
        "        augmented_image = augmenters(image=image)\n",
        "\n",
        "        # Extraer ID del paciente correctamente\n",
        "        patient_id = \"_\".join(img_name.split(\"_\")[:3])  # Extrae \"OAS1_0028_MR1\"\n",
        "        slice_info = \"_\".join(img_name.split(\"_\")[3:])  # Extrae \"1.nii_slice_113\"\n",
        "\n",
        "        # Nuevo nombre con ID del paciente y slice preservados\n",
        "        aug_img_name = f\"{patient_id}_aug{i}_{slice_info}\"\n",
        "\n",
        "        cv2.imwrite(os.path.join(new_category_path, aug_img_name), cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print(\"Data augmentation completado SIN la clase 'ModerateDemented'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9fptxkHRrFR",
        "outputId": "0647a069-73f1-4a93-906e-8eb55ab2204e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation completado SIN la clase 'ModerateDemented'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_augmented_images(augmented_dir):\n",
        "    \"\"\"Cuenta cuántas imágenes hay en cada subcarpeta dentro de augmented_dir.\"\"\"\n",
        "    folder_counts = {}\n",
        "\n",
        "    for category in os.listdir(augmented_dir):\n",
        "        category_path = os.path.join(augmented_dir, category)\n",
        "        if os.path.isdir(category_path):\n",
        "            image_count = len([img for img in os.listdir(category_path) if img.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "            folder_counts[category] = image_count\n",
        "\n",
        "    return folder_counts\n",
        "\n",
        "# Ruta de la carpeta con las imágenes aumentadas\n",
        "augmented_dir = \"/root/.cache/kagglehub/datasets/augmented_dataset\"\n",
        "\n",
        "# Contar imágenes\n",
        "augmented_counts = count_augmented_images(augmented_dir)\n",
        "\n",
        "# Imprimir resultados\n",
        "for category, count in augmented_counts.items():\n",
        "    print(f\"{category}: {count} imágenes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "traKGaFmTHG3",
        "outputId": "01cac3ae-b904-42e3-f8c0-075e96fec42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VeryMildDemented: 1204 imágenes\n",
            "MildDemented: 2816 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directorios base\n",
        "original_base_dir = path\n",
        "augmented_base_dir = \"/root/.cache/kagglehub/datasets/augmented_dataset\"\n",
        "\n",
        "# Categorías sin \"ModerateDemented\"\n",
        "category_subfolders = {\n",
        "    \"MildDemented\": \"MildDemented\",\n",
        "    \"VeryMildDemented\": \"VeryMildDemented\",\n",
        "    \"NonDemented (2)\": \"NonDemented\"\n",
        "}\n",
        "\n",
        "# Listas para almacenar la información de las imágenes\n",
        "image_paths = []\n",
        "patient_ids = []\n",
        "labels = []\n",
        "\n",
        "# Función para extraer el ID del paciente desde el nombre del archivo\n",
        "def extract_patient_id(img_name):\n",
        "    return \"_\".join(img_name.split(\"_\")[:3])  # Ejemplo: \"OAS1_0028_MR1\"\n",
        "\n",
        "# Cargar imágenes originales\n",
        "for category, subfolder in category_subfolders.items():\n",
        "    category_path = os.path.join(original_base_dir, category, subfolder)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Advertencia: No se encontró la carpeta {category_path}, se omitirá.\")\n",
        "        continue\n",
        "\n",
        "    for img_name in os.listdir(category_path):\n",
        "        if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            patient_id = extract_patient_id(img_name)\n",
        "            image_paths.append(os.path.join(category_path, img_name))\n",
        "            patient_ids.append(patient_id)\n",
        "            labels.append(category)\n",
        "\n",
        "# Cargar imágenes aumentadas\n",
        "for category in category_subfolders.keys():\n",
        "    category_path = os.path.join(augmented_base_dir, category)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Advertencia: No se encontró la carpeta de aumentación {category_path}, se omitirá.\")\n",
        "        continue\n",
        "\n",
        "    for img_name in os.listdir(category_path):\n",
        "        if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            patient_id = extract_patient_id(img_name)\n",
        "            image_paths.append(os.path.join(category_path, img_name))\n",
        "            patient_ids.append(patient_id)\n",
        "            labels.append(category)\n",
        "\n",
        "# Crear DataFrame con todas las imágenes\n",
        "images_df = pd.DataFrame({\n",
        "    \"image_path\": image_paths,\n",
        "    \"patient_id\": patient_ids,\n",
        "    \"label\": labels\n",
        "})\n",
        "\n",
        "# Obtener etiquetas de pacientes para estratificación\n",
        "patient_labels = images_df.groupby(\"patient_id\")[\"label\"].first()\n",
        "\n",
        "# Obtener IDs únicos de pacientes\n",
        "unique_patient_ids = images_df[\"patient_id\"].unique()\n",
        "\n",
        "# División en train (80%) y test (20%) por pacientes\n",
        "train_patient_ids, test_patient_ids = train_test_split(\n",
        "    unique_patient_ids, test_size=0.2, random_state=42, stratify=patient_labels.loc[unique_patient_ids]\n",
        ")\n",
        "\n",
        "# División de train en train (80%) y validación (20%)\n",
        "train_patient_ids, val_patient_ids = train_test_split(\n",
        "    train_patient_ids, test_size=0.2, random_state=42, stratify=patient_labels.loc[train_patient_ids]\n",
        ")\n",
        "\n",
        "# Asignar cada imagen a su conjunto correspondiente\n",
        "final_df = images_df.copy()\n",
        "final_df[\"set\"] = final_df[\"patient_id\"].apply(\n",
        "    lambda pid: \"train\" if pid in train_patient_ids else (\"val\" if pid in val_patient_ids else \"test\")\n",
        ")\n",
        "\n",
        "# Verificar la distribución final\n",
        "print(f\"Tamaño de entrenamiento: {len(final_df[final_df['set'] == 'train'])}\")\n",
        "print(f\"Tamaño de validación: {len(final_df[final_df['set'] == 'val'])}\")\n",
        "print(f\"Tamaño de prueba: {len(final_df[final_df['set'] == 'test'])}\")\n",
        "\n",
        "# Mostrar algunas filas del DataFrame final\n",
        "print(final_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DebqZIwjXwBa",
        "outputId": "3c879f76-aab3-4d25-b000-10d5cc98dbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advertencia: No se encontró la carpeta de aumentación /root/.cache/kagglehub/datasets/augmented_dataset/NonDemented (2), se omitirá.\n",
            "Tamaño de entrenamiento: 55458\n",
            "Tamaño de validación: 13397\n",
            "Tamaño de prueba: 17705\n",
            "                                          image_path     patient_id  \\\n",
            "0  /root/.cache/kagglehub/datasets/yiweilu2033/we...  OAS1_0424_MR1   \n",
            "1  /root/.cache/kagglehub/datasets/yiweilu2033/we...  OAS1_0382_MR1   \n",
            "2  /root/.cache/kagglehub/datasets/yiweilu2033/we...  OAS1_0316_MR1   \n",
            "3  /root/.cache/kagglehub/datasets/yiweilu2033/we...  OAS1_0031_MR1   \n",
            "4  /root/.cache/kagglehub/datasets/yiweilu2033/we...  OAS1_0035_MR1   \n",
            "\n",
            "          label    set  \n",
            "0  MildDemented  train  \n",
            "1  MildDemented  train  \n",
            "2  MildDemented  train  \n",
            "3  MildDemented  train  \n",
            "4  MildDemented  train  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Transformaciones estándar para todas las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                 # Redimensionar la imagen\n",
        "    transforms.RandomHorizontalFlip(p=0.5),        # Volteo aleatorio horizontal\n",
        "    transforms.RandomRotation(degrees=15),         # Rotación aleatoria\n",
        "    transforms.ToTensor(),                         # Convertir la imagen a tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalización\n",
        "])\n",
        "\n",
        "# Mapeo de etiquetas a números (sin \"ModerateDemented\")\n",
        "label_map = {\"MildDemented\": 0, \"VeryMildDemented\": 1, \"NonDemented (2)\": 2}\n",
        "\n",
        "class AlzheimerDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        \"\"\"\n",
        "        Inicializa el dataset de Alzheimer.\n",
        "\n",
        "        Args:\n",
        "        df (pd.DataFrame): DataFrame con las rutas de imágenes y etiquetas.\n",
        "        transform (callable, optional): Transformaciones a aplicar a cada imagen.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Devuelve el número total de imágenes en el dataset.\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Obtiene una imagen y su etiqueta correspondiente del DataFrame.\n",
        "\n",
        "        Args:\n",
        "        idx (int): Índice de la muestra.\n",
        "\n",
        "        Returns:\n",
        "        image (Tensor): Imagen transformada.\n",
        "        label (int): Etiqueta de la imagen.\n",
        "        \"\"\"\n",
        "        img_path = self.df.iloc[idx][\"image_path\"]\n",
        "        label = label_map[self.df.iloc[idx][\"label\"]]\n",
        "\n",
        "        # Cargar la imagen\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Aplicar transformaciones\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Filtrar el DataFrame para excluir \"ModerateDemented\"\n",
        "filtered_df = final_df[final_df[\"label\"] != \"ModerateDemented\"]\n",
        "\n",
        "# Filtrar los DataFrames por cada conjunto\n",
        "train_df = filtered_df[filtered_df[\"set\"] == \"train\"]\n",
        "val_df = filtered_df[filtered_df[\"set\"] == \"val\"]\n",
        "test_df = filtered_df[filtered_df[\"set\"] == \"test\"]\n",
        "\n",
        "# Crear los datasets de PyTorch para cada conjunto\n",
        "train_dataset = AlzheimerDataset(train_df, transform=transform)\n",
        "val_dataset = AlzheimerDataset(val_df, transform=transform)\n",
        "test_dataset = AlzheimerDataset(test_df, transform=transform)\n",
        "\n",
        "# Imprimir el tamaño de cada dataset\n",
        "print(f\"Tamaño del dataset -> Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# Crear los DataLoaders para cargar los datos por lotes\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"DataLoaders creados con batch_size={batch_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCK_67hfYHX2",
        "outputId": "3126965f-669c-41e4-861b-4f7b5bf05aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del dataset -> Train: 55458, Val: 13397, Test: 17705\n",
            "DataLoaders creados con batch_size=128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AlzheimerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):  # Ahora tenemos 3 clases en lugar de 4\n",
        "        super(AlzheimerCNN, self).__init__()\n",
        "\n",
        "        # Bloque 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bloque 2\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calcular el tamaño de entrada para la capa densa\n",
        "        self.flattened_size = self._get_flattened_size()\n",
        "\n",
        "        # Capas densas\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)  # Ajustado a 3 clases\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        \"\"\"Calcula automáticamente el tamaño de la salida antes de la capa fully connected.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, 224, 224)  # Imagen de ejemplo\n",
        "            x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(dummy_input)))))\n",
        "            x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "            return x.view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Inicializar modelo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AlzheimerCNN(num_classes=3).to(device)  # Se pasa el nuevo número de clases\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y60s_LFgYYHS",
        "outputId": "e93b9dc1-45f4-436f-b3af-783e05bf04a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlzheimerCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=100352, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Definir la función de pérdida y el optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler para reducir el LR si la validación no mejora\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "# 2. Función para entrenar el modelo (SIN Early Stopping)\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_preds / total_preds\n",
        "\n",
        "        # Evaluación en validación\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_preds_val = 0\n",
        "        total_preds_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct_preds_val += torch.sum(preds == labels)\n",
        "                total_preds_val += labels.size(0)\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = correct_preds_val / total_preds_val\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_acc)\n",
        "\n",
        "        # Reducir la tasa de aprendizaje si la validación no mejora\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Guardar el mejor modelo basado en la pérdida de validación\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\" Mejor modelo guardado en la época {epoch+1} con val_loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        # Imprimir resultados de la época\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f} - \"\n",
        "              f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "# 3. Función para evaluar el modelo en el conjunto de prueba\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    model.eval()\n",
        "    correct_preds_test = 0\n",
        "    total_preds_test = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            correct_preds_test += torch.sum(preds == labels)\n",
        "            total_preds_test += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_accuracy = correct_preds_test / total_preds_test\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Test Accuracy (sklearn): {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# 4. Entrenar el modelo (SIN Early Stopping)\n",
        "num_epochs = 25\n",
        "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "# 5. Evaluación final en el conjunto de prueba\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "\n",
        "# 6. Guardar el mejor modelo entrenado\n",
        "model_save_path = \"paper2_aug.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Modelo guardado en {model_save_path}\")\n",
        "files.download(model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rANZExQ4YhNy",
        "outputId": "55bc0e77-fa30-4b9c-f37b-221ca2bbb1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mejor modelo guardado en la época 1 con val_loss: 0.5375\n",
            "Epoch 1/25 - Train Loss: 0.5259, Train Accuracy: 0.7609 - Validation Loss: 0.5375, Validation Accuracy: 0.7556\n",
            " Mejor modelo guardado en la época 2 con val_loss: 0.5247\n",
            "Epoch 2/25 - Train Loss: 0.4515, Train Accuracy: 0.7795 - Validation Loss: 0.5247, Validation Accuracy: 0.7582\n",
            "Epoch 3/25 - Train Loss: 0.4149, Train Accuracy: 0.7989 - Validation Loss: 0.5838, Validation Accuracy: 0.7531\n",
            "Epoch 4/25 - Train Loss: 0.3881, Train Accuracy: 0.8184 - Validation Loss: 0.5682, Validation Accuracy: 0.7676\n",
            "Epoch 5/25 - Train Loss: 0.3532, Train Accuracy: 0.8378 - Validation Loss: 0.5839, Validation Accuracy: 0.7663\n",
            "Epoch 6/25 - Train Loss: 0.3361, Train Accuracy: 0.8487 - Validation Loss: 0.6104, Validation Accuracy: 0.7384\n",
            "Epoch 7/25 - Train Loss: 0.2773, Train Accuracy: 0.8789 - Validation Loss: 0.7680, Validation Accuracy: 0.7565\n",
            "Epoch 8/25 - Train Loss: 0.2518, Train Accuracy: 0.8925 - Validation Loss: 0.7259, Validation Accuracy: 0.7396\n",
            "Epoch 9/25 - Train Loss: 0.2348, Train Accuracy: 0.9005 - Validation Loss: 0.7819, Validation Accuracy: 0.7514\n",
            "Epoch 10/25 - Train Loss: 0.2162, Train Accuracy: 0.9100 - Validation Loss: 0.8077, Validation Accuracy: 0.7488\n",
            "Epoch 11/25 - Train Loss: 0.1888, Train Accuracy: 0.9229 - Validation Loss: 0.8436, Validation Accuracy: 0.7569\n",
            "Epoch 12/25 - Train Loss: 0.1752, Train Accuracy: 0.9290 - Validation Loss: 0.9192, Validation Accuracy: 0.7585\n",
            "Epoch 13/25 - Train Loss: 0.1690, Train Accuracy: 0.9327 - Validation Loss: 0.9293, Validation Accuracy: 0.7470\n",
            "Epoch 14/25 - Train Loss: 0.1626, Train Accuracy: 0.9345 - Validation Loss: 0.9948, Validation Accuracy: 0.7607\n",
            "Epoch 15/25 - Train Loss: 0.1452, Train Accuracy: 0.9430 - Validation Loss: 0.9902, Validation Accuracy: 0.7462\n",
            "Epoch 16/25 - Train Loss: 0.1391, Train Accuracy: 0.9466 - Validation Loss: 1.0291, Validation Accuracy: 0.7532\n",
            "Epoch 17/25 - Train Loss: 0.1275, Train Accuracy: 0.9495 - Validation Loss: 1.0419, Validation Accuracy: 0.7466\n",
            "Epoch 18/25 - Train Loss: 0.1272, Train Accuracy: 0.9512 - Validation Loss: 1.1356, Validation Accuracy: 0.7475\n",
            "Epoch 19/25 - Train Loss: 0.1172, Train Accuracy: 0.9559 - Validation Loss: 1.1147, Validation Accuracy: 0.7532\n",
            "Epoch 20/25 - Train Loss: 0.1145, Train Accuracy: 0.9577 - Validation Loss: 1.0568, Validation Accuracy: 0.7514\n",
            "Epoch 21/25 - Train Loss: 0.1107, Train Accuracy: 0.9582 - Validation Loss: 1.2296, Validation Accuracy: 0.7483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Crear la matriz de confusión\n",
        "labels_classes = [\"MildDemented\", \"VeryMildDemented\", \"NonDemented\"]  # Eliminado \"ModerateDemented\"\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Graficar la matriz de confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels_classes, yticklabels=labels_classes)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c1pRNDasYwcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertir las etiquetas en formato binario (one-vs-all) para 3 clases\n",
        "y_true_bin = label_binarize(y_true, classes=[0, 1, 2])  # Solo 3 clases ahora\n",
        "y_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)  # Obtiene los logits (valores antes de softmax)\n",
        "        y_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "y_scores = np.array(y_scores)\n",
        "\n",
        "# Graficar la curva ROC para cada clase\n",
        "plt.figure(figsize=(8,6))\n",
        "labels_classes = [\"MildDemented\", \"VeryMildDemented\", \"NonDemented\"]  # Sin \"ModerateDemented\"\n",
        "\n",
        "for i in range(3):  # Ahora solo 3 clases\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Clase {labels_classes[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Curvas ROC por Clase\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EgUvlFI0Yvy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}