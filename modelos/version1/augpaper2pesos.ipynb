{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irh3NxmfHwZf",
        "outputId": "5101371e-a67a-4675-a120-fee5ee2bdd06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/well-documented-alzheimers-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"yiweilu2033/well-documented-alzheimers-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzVRn0FgHzkQ",
        "outputId": "2d33078f-61bd-4359-c037-cab5e3844d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ModerateDemented', 'NonDemented (2)', 'oasis_cross-sectional-5708aa0a98d82080 (1).xlsx', 'VeryMildDemented', 'MildDemented']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def count_images_in_folders(dataset_path):\n",
        "    folder_counts = {}\n",
        "\n",
        "    for folder in os.listdir(dataset_path):\n",
        "        folder_path = os.path.join(dataset_path, folder)\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Buscar subcarpeta con el mismo nombre (excepto para 'NonDemented')\n",
        "            subfolder_name = folder if \"NonDemented\" not in folder else \"NonDemented\"\n",
        "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
        "\n",
        "            if os.path.isdir(subfolder_path):\n",
        "                image_count = len([f for f in os.listdir(subfolder_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))])\n",
        "                folder_counts[folder] = image_count\n",
        "            else:\n",
        "                print(f\"Subcarpeta no encontrada en: {folder_path}\")\n",
        "\n",
        "    return folder_counts\n",
        "\n",
        "# Ruta del dataset\n",
        "image_counts = count_images_in_folders(path)\n",
        "\n",
        "# Imprimir resultados\n",
        "for folder, count in image_counts.items():\n",
        "    print(f\"{folder}: {count} imágenes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClhuBAWuH4La",
        "outputId": "e24e2a7a-0e8c-4af6-8520-62fe99a1a612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModerateDemented: 376 imágenes\n",
            "NonDemented (2): 63560 imágenes\n",
            "VeryMildDemented: 13796 imágenes\n",
            "MildDemented: 5184 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imgaug\n",
        "!pip install numpy==1.24.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "id": "V76m_rHpRwBT",
        "outputId": "947b74be-dfa5-434a-f3ff-9733f5230017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imgaug\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imgaug) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from imgaug) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from imgaug) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.11/dist-packages (from imgaug) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from imgaug) (4.11.0.86)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.37.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.1.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (2.8.2)\n",
            "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imgaug\n",
            "Successfully installed imgaug-0.4.0\n",
            "Collecting numpy==1.24.0\n",
            "  Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "3337df66904e48ee826b4c3cf33c8f33"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imgaug.augmenters as iaa\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Configurar las transformaciones\n",
        "augmenters = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5),  # Volteo horizontal\n",
        "    iaa.Affine(rotate=(-15, 15)),  # Rotación aleatoria\n",
        "    iaa.GaussianBlur(sigma=(0, 1.0)),  # Desenfoque aleatorio\n",
        "])\n",
        "\n",
        "# Directorio donde guardar imágenes aumentadas\n",
        "augmented_dir = \"/root/.cache/kagglehub/datasets/augmented_dataset\"\n",
        "os.makedirs(augmented_dir, exist_ok=True)\n",
        "\n",
        "# Directorio base donde están las imágenes\n",
        "base_dir = path\n",
        "\n",
        "# Diccionario con la estructura correcta de carpetas y subcarpetas (sin ModerateDemented)\n",
        "category_subfolders = {\n",
        "    \"MildDemented\": \"MildDemented\",\n",
        "    \"VeryMildDemented\": \"VeryMildDemented\",\n",
        "    \"NonDemented (2)\": \"NonDemented\"\n",
        "}\n",
        "\n",
        "# Número de imágenes objetivo por clase (sin ModerateDemented)\n",
        "target_counts = {\n",
        "    \"MildDemented\": 8000,\n",
        "    \"VeryMildDemented\": 15000,\n",
        "    \"NonDemented\": None  # No se aumenta esta categoría\n",
        "}\n",
        "\n",
        "for category, subfolder in category_subfolders.items():\n",
        "    category_path = os.path.join(base_dir, category, subfolder)\n",
        "\n",
        "    if not os.path.exists(category_path) or category not in target_counts:\n",
        "        continue\n",
        "\n",
        "    images = [img for img in os.listdir(category_path) if img.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
        "\n",
        "    if target_counts[category] is None or len(images) >= target_counts[category]:\n",
        "        continue  # No se hace augmentación si ya hay suficientes imágenes\n",
        "\n",
        "    new_category_path = os.path.join(augmented_dir, category)\n",
        "    os.makedirs(new_category_path, exist_ok=True)\n",
        "\n",
        "    augmentation_needed = target_counts[category] - len(images)\n",
        "\n",
        "    for i in range(augmentation_needed):\n",
        "        img_name = images[i % len(images)]  # Seleccionar imágenes repetidamente si es necesario\n",
        "        img_path = os.path.join(category_path, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Aplicar augmentación\n",
        "        augmented_image = augmenters(image=image)\n",
        "\n",
        "        # Extraer ID del paciente correctamente\n",
        "        patient_id = \"_\".join(img_name.split(\"_\")[:3])  # Extrae \"OAS1_0028_MR1\"\n",
        "        slice_info = \"_\".join(img_name.split(\"_\")[3:])  # Extrae \"1.nii_slice_113\"\n",
        "\n",
        "        # Nuevo nombre con ID del paciente y slice preservados\n",
        "        aug_img_name = f\"{patient_id}_aug{i}_{slice_info}\"\n",
        "\n",
        "        cv2.imwrite(os.path.join(new_category_path, aug_img_name), cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print(\"Data augmentation completado SIN la clase 'ModerateDemented'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9fptxkHRrFR",
        "outputId": "5c4c0f01-a62f-4645-8913-767aa323dd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation completado SIN la clase 'ModerateDemented'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_augmented_images(augmented_dir):\n",
        "    \"\"\"Cuenta cuántas imágenes hay en cada subcarpeta dentro de augmented_dir.\"\"\"\n",
        "    folder_counts = {}\n",
        "\n",
        "    for category in os.listdir(augmented_dir):\n",
        "        category_path = os.path.join(augmented_dir, category)\n",
        "        if os.path.isdir(category_path):\n",
        "            image_count = len([img for img in os.listdir(category_path) if img.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "            folder_counts[category] = image_count\n",
        "\n",
        "    return folder_counts\n",
        "\n",
        "# Ruta de la carpeta con las imágenes aumentadas\n",
        "augmented_dir = \"/root/.cache/kagglehub/datasets/augmented_dataset\"\n",
        "\n",
        "# Contar imágenes\n",
        "augmented_counts = count_augmented_images(augmented_dir)\n",
        "\n",
        "# Imprimir resultados\n",
        "for category, count in augmented_counts.items():\n",
        "    print(f\"{category}: {count} imágenes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "traKGaFmTHG3",
        "outputId": "f29d6479-073d-4a82-8ace-d8156eecdc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VeryMildDemented: 1204 imágenes\n",
            "MildDemented: 2816 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directorios base\n",
        "original_base_dir = path\n",
        "augmented_base_dir = \"/root/.cache/kagglehub/datasets/augmented_dataset\"\n",
        "\n",
        "# Categorías sin \"ModerateDemented\"\n",
        "category_subfolders = {\n",
        "    \"MildDemented\": \"MildDemented\",\n",
        "    \"VeryMildDemented\": \"VeryMildDemented\",\n",
        "    \"NonDemented (2)\": \"NonDemented\"\n",
        "}\n",
        "\n",
        "# Listas para almacenar la información de las imágenes\n",
        "image_paths = []\n",
        "patient_ids = []\n",
        "labels = []\n",
        "\n",
        "# Función para extraer el ID del paciente desde el nombre del archivo\n",
        "def extract_patient_id(img_name):\n",
        "    return \"_\".join(img_name.split(\"_\")[:3])  # Ejemplo: \"OAS1_0028_MR1\"\n",
        "\n",
        "# Cargar imágenes originales\n",
        "for category, subfolder in category_subfolders.items():\n",
        "    category_path = os.path.join(original_base_dir, category, subfolder)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Advertencia: No se encontró la carpeta {category_path}, se omitirá.\")\n",
        "        continue\n",
        "\n",
        "    for img_name in os.listdir(category_path):\n",
        "        if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            patient_id = extract_patient_id(img_name)\n",
        "            image_paths.append(os.path.join(category_path, img_name))\n",
        "            patient_ids.append(patient_id)\n",
        "            labels.append(category)\n",
        "\n",
        "# Cargar imágenes aumentadas\n",
        "for category in category_subfolders.keys():\n",
        "    category_path = os.path.join(augmented_base_dir, category)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Advertencia: No se encontró la carpeta de aumentación {category_path}, se omitirá.\")\n",
        "        continue\n",
        "\n",
        "    for img_name in os.listdir(category_path):\n",
        "        if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            patient_id = extract_patient_id(img_name)\n",
        "            image_paths.append(os.path.join(category_path, img_name))\n",
        "            patient_ids.append(patient_id)\n",
        "            labels.append(category)\n",
        "\n",
        "# Crear DataFrame con todas las imágenes\n",
        "images_df = pd.DataFrame({\n",
        "    \"image_path\": image_paths,\n",
        "    \"patient_id\": patient_ids,\n",
        "    \"label\": labels\n",
        "})\n",
        "\n",
        "# Obtener etiquetas de pacientes para estratificación\n",
        "patient_labels = images_df.groupby(\"patient_id\")[\"label\"].first()\n",
        "\n",
        "# Obtener IDs únicos de pacientes\n",
        "unique_patient_ids = images_df[\"patient_id\"].unique()\n",
        "\n",
        "# División en train (80%) y test (20%) por pacientes\n",
        "train_patient_ids, test_patient_ids = train_test_split(\n",
        "    unique_patient_ids, test_size=0.2, random_state=42, stratify=patient_labels.loc[unique_patient_ids]\n",
        ")\n",
        "\n",
        "# División de train en train (80%) y validación (20%)\n",
        "train_patient_ids, val_patient_ids = train_test_split(\n",
        "    train_patient_ids, test_size=0.2, random_state=42, stratify=patient_labels.loc[train_patient_ids]\n",
        ")\n",
        "\n",
        "# Asignar cada imagen a su conjunto correspondiente\n",
        "final_df = images_df.copy()\n",
        "final_df[\"set\"] = final_df[\"patient_id\"].apply(\n",
        "    lambda pid: \"train\" if pid in train_patient_ids else (\"val\" if pid in val_patient_ids else \"test\")\n",
        ")\n",
        "\n",
        "# Verificar la distribución final\n",
        "print(f\"Tamaño de entrenamiento: {len(final_df[final_df['set'] == 'train'])}\")\n",
        "print(f\"Tamaño de validación: {len(final_df[final_df['set'] == 'val'])}\")\n",
        "print(f\"Tamaño de prueba: {len(final_df[final_df['set'] == 'test'])}\")\n",
        "\n",
        "# Mostrar algunas filas del DataFrame final\n",
        "print(final_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DebqZIwjXwBa",
        "outputId": "8e7bf441-612c-4c8d-dfab-4ea32a793153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advertencia: No se encontró la carpeta de aumentación /root/.cache/kagglehub/datasets/augmented_dataset/NonDemented (2), se omitirá.\n",
            "Tamaño de entrenamiento: 54773\n",
            "Tamaño de validación: 14014\n",
            "Tamaño de prueba: 17773\n",
            "                                          image_path     patient_id  \\\n",
            "0  /kaggle/input/well-documented-alzheimers-datas...  OAS1_0073_MR1   \n",
            "1  /kaggle/input/well-documented-alzheimers-datas...  OAS1_0316_MR1   \n",
            "2  /kaggle/input/well-documented-alzheimers-datas...  OAS1_0430_MR1   \n",
            "3  /kaggle/input/well-documented-alzheimers-datas...  OAS1_0184_MR1   \n",
            "4  /kaggle/input/well-documented-alzheimers-datas...  OAS1_0269_MR1   \n",
            "\n",
            "          label    set  \n",
            "0  MildDemented  train  \n",
            "1  MildDemented  train  \n",
            "2  MildDemented  train  \n",
            "3  MildDemented  train  \n",
            "4  MildDemented  train  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Transformaciones estándar para todas las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                 # Redimensionar la imagen\n",
        "    transforms.RandomHorizontalFlip(p=0.5),        # Volteo aleatorio horizontal\n",
        "    transforms.RandomRotation(degrees=15),         # Rotación aleatoria\n",
        "    transforms.ToTensor(),                         # Convertir la imagen a tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalización\n",
        "])\n",
        "\n",
        "# Mapeo de etiquetas a números (sin \"ModerateDemented\")\n",
        "label_map = {\"MildDemented\": 0, \"VeryMildDemented\": 1, \"NonDemented (2)\": 2}\n",
        "\n",
        "class AlzheimerDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        \"\"\"\n",
        "        Inicializa el dataset de Alzheimer.\n",
        "\n",
        "        Args:\n",
        "        df (pd.DataFrame): DataFrame con las rutas de imágenes y etiquetas.\n",
        "        transform (callable, optional): Transformaciones a aplicar a cada imagen.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Devuelve el número total de imágenes en el dataset.\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Obtiene una imagen y su etiqueta correspondiente del DataFrame.\n",
        "\n",
        "        Args:\n",
        "        idx (int): Índice de la muestra.\n",
        "\n",
        "        Returns:\n",
        "        image (Tensor): Imagen transformada.\n",
        "        label (int): Etiqueta de la imagen.\n",
        "        \"\"\"\n",
        "        img_path = self.df.iloc[idx][\"image_path\"]\n",
        "        label = label_map[self.df.iloc[idx][\"label\"]]\n",
        "\n",
        "        # Cargar la imagen\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Aplicar transformaciones\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Filtrar el DataFrame para excluir \"ModerateDemented\"\n",
        "filtered_df = final_df[final_df[\"label\"] != \"ModerateDemented\"]\n",
        "\n",
        "# Filtrar los DataFrames por cada conjunto\n",
        "train_df = filtered_df[filtered_df[\"set\"] == \"train\"]\n",
        "val_df = filtered_df[filtered_df[\"set\"] == \"val\"]\n",
        "test_df = filtered_df[filtered_df[\"set\"] == \"test\"]\n",
        "\n",
        "# Crear los datasets de PyTorch para cada conjunto\n",
        "train_dataset = AlzheimerDataset(train_df, transform=transform)\n",
        "val_dataset = AlzheimerDataset(val_df, transform=transform)\n",
        "test_dataset = AlzheimerDataset(test_df, transform=transform)\n",
        "\n",
        "# Imprimir el tamaño de cada dataset\n",
        "print(f\"Tamaño del dataset -> Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# Crear los DataLoaders para cargar los datos por lotes\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"DataLoaders creados con batch_size={batch_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCK_67hfYHX2",
        "outputId": "594eaf25-edbf-42c1-f8e2-2e7023a1ea10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del dataset -> Train: 54773, Val: 14014, Test: 17773\n",
            "DataLoaders creados con batch_size=128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Definir el dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cálculo de pesos por clase\n",
        "label_to_idx = {\n",
        "    \"MildDemented\": 0,\n",
        "    \"VeryMildDemented\": 1,\n",
        "    \"NonDemented (2)\": 2\n",
        "}\n",
        "\n",
        "# Contar frecuencia de clases en el conjunto de entrenamiento\n",
        "label_counts = Counter([label_to_idx[label] for label in final_df[final_df[\"set\"] == \"train\"][\"label\"]])\n",
        "total_samples = sum(label_counts.values())\n",
        "\n",
        "# Peso inversamente proporcional a la frecuencia de cada clase\n",
        "class_weights = [0] * len(label_to_idx)\n",
        "for label, idx in label_to_idx.items():\n",
        "    class_weights[idx] = total_samples / (len(label_to_idx) * label_counts[idx])\n",
        "\n",
        "# Convertir los pesos a tensor para PyTorch y mover al dispositivo\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Definir la función de pérdida con pesos\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Ahora `criterion` puede ser utilizado en tu modelo\n"
      ],
      "metadata": {
        "id": "m-E1TlzusgoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AlzheimerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):  # Ahora tenemos 3 clases en lugar de 4\n",
        "        super(AlzheimerCNN, self).__init__()\n",
        "\n",
        "        # Bloque 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bloque 2\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calcular el tamaño de entrada para la capa densa\n",
        "        self.flattened_size = self._get_flattened_size()\n",
        "\n",
        "        # Capas densas\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)  # Ajustado a 3 clases\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        \"\"\"Calcula automáticamente el tamaño de la salida antes de la capa fully connected.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, 224, 224)  # Imagen de ejemplo\n",
        "            x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(dummy_input)))))\n",
        "            x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "            return x.view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Inicializar modelo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AlzheimerCNN(num_classes=3).to(device)  # Se pasa el nuevo número de clases\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y60s_LFgYYHS",
        "outputId": "9cdc7ac0-bccd-44a0-ed36-9121796063b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlzheimerCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=100352, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "# Calcular pesos de clase con base en train_df\n",
        "from collections import Counter\n",
        "i\n",
        "# Cuenta cuántas instancias hay por clase en train_df\n",
        "class_counts = Counter(train_df[\"label\"])\n",
        "total_samples = sum(class_counts.values())\n",
        "\n",
        "# Asegúrate de que las clases estén en el orden correcto según label_map\n",
        "weights = []\n",
        "for label in [\"MildDemented\", \"VeryMildDemented\", \"NonDemented (2)\"]:\n",
        "    count = class_counts[label]\n",
        "    weights.append(total_samples / count)\n",
        "\n",
        "# Convertir a tensor de PyTorch normalizado\n",
        "weights = torch.tensor(weights, dtype=torch.float32)\n",
        "weights = weights / weights.sum()  # Normalizamos para estabilidad\n",
        "weights = weights.to(device)\n",
        "\n",
        "#1. Definir la función de pérdida ponderada y el optimizador\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 2. Función para entrenar el modelo (SIN Early Stopping)\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_preds / total_preds\n",
        "\n",
        "        # Evaluación en validación\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_preds_val = 0\n",
        "        total_preds_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct_preds_val += torch.sum(preds == labels)\n",
        "                total_preds_val += labels.size(0)\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = correct_preds_val / total_preds_val\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_acc)\n",
        "\n",
        "        # Guardar el mejor modelo basado en la pérdida de validación\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\" Mejor modelo guardado en la época {epoch+1} con val_loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        # Imprimir resultados de la época\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f} - \"\n",
        "              f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "# 3. Función para evaluar el modelo en el conjunto de prueba\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    model.eval()\n",
        "    correct_preds_test = 0\n",
        "    total_preds_test = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            correct_preds_test += torch.sum(preds == labels)\n",
        "            total_preds_test += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_accuracy = correct_preds_test / total_preds_test\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Test Accuracy (sklearn): {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# 4. Entrenar el modelo (SIN Early Stopping)\n",
        "num_epochs = 25\n",
        "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "# 5. Evaluación final en el conjunto de prueba\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "\n",
        "# 6. Guardar el mejor modelo entrenado\n",
        "model_save_path = \"modelopaper2.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Modelo guardado en {model_save_path}\")\n",
        "files.download(model_save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rANZExQ4YhNy",
        "outputId": "6f588f09-b573-4ac9-f149-f15437d25a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mejor modelo guardado en la época 1 con val_loss: 0.8058\n",
            "Epoch 1/25 - Train Loss: 0.8964, Train Accuracy: 0.6066 - Validation Loss: 0.8058, Validation Accuracy: 0.5933\n",
            " Mejor modelo guardado en la época 2 con val_loss: 0.5811\n",
            "Epoch 2/25 - Train Loss: 0.7042, Train Accuracy: 0.7269 - Validation Loss: 0.5811, Validation Accuracy: 0.7077\n",
            "Epoch 3/25 - Train Loss: 0.6223, Train Accuracy: 0.7512 - Validation Loss: 0.6002, Validation Accuracy: 0.6929\n",
            "Epoch 4/25 - Train Loss: 0.5555, Train Accuracy: 0.7743 - Validation Loss: 0.7093, Validation Accuracy: 0.6593\n",
            "Epoch 5/25 - Train Loss: 0.5086, Train Accuracy: 0.7906 - Validation Loss: 0.6826, Validation Accuracy: 0.7035\n",
            "Epoch 6/25 - Train Loss: 0.4765, Train Accuracy: 0.8006 - Validation Loss: 0.7284, Validation Accuracy: 0.6797\n",
            "Epoch 7/25 - Train Loss: 0.4442, Train Accuracy: 0.8101 - Validation Loss: 0.6721, Validation Accuracy: 0.7045\n",
            "Epoch 8/25 - Train Loss: 0.4157, Train Accuracy: 0.8203 - Validation Loss: 0.7307, Validation Accuracy: 0.6733\n",
            "Epoch 9/25 - Train Loss: 0.3980, Train Accuracy: 0.8247 - Validation Loss: 0.7435, Validation Accuracy: 0.6824\n",
            "Epoch 10/25 - Train Loss: 0.3815, Train Accuracy: 0.8308 - Validation Loss: 0.7150, Validation Accuracy: 0.7333\n",
            "Epoch 11/25 - Train Loss: 0.3631, Train Accuracy: 0.8383 - Validation Loss: 0.7468, Validation Accuracy: 0.6975\n",
            "Epoch 12/25 - Train Loss: 0.3493, Train Accuracy: 0.8422 - Validation Loss: 0.7601, Validation Accuracy: 0.6805\n",
            "Epoch 13/25 - Train Loss: 0.3389, Train Accuracy: 0.8437 - Validation Loss: 0.7489, Validation Accuracy: 0.7136\n",
            "Epoch 14/25 - Train Loss: 0.3353, Train Accuracy: 0.8448 - Validation Loss: 0.7914, Validation Accuracy: 0.6753\n",
            "Epoch 15/25 - Train Loss: 0.3186, Train Accuracy: 0.8498 - Validation Loss: 0.7900, Validation Accuracy: 0.6841\n",
            "Epoch 16/25 - Train Loss: 0.3011, Train Accuracy: 0.8571 - Validation Loss: 0.7668, Validation Accuracy: 0.7123\n",
            "Epoch 17/25 - Train Loss: 0.2929, Train Accuracy: 0.8614 - Validation Loss: 0.7532, Validation Accuracy: 0.7153\n",
            "Epoch 18/25 - Train Loss: 0.2873, Train Accuracy: 0.8656 - Validation Loss: 0.7489, Validation Accuracy: 0.7071\n",
            "Epoch 19/25 - Train Loss: 0.2773, Train Accuracy: 0.8691 - Validation Loss: 0.7586, Validation Accuracy: 0.7252\n",
            "Epoch 20/25 - Train Loss: 0.2689, Train Accuracy: 0.8714 - Validation Loss: 0.8115, Validation Accuracy: 0.7006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Crear la matriz de confusión\n",
        "labels_classes = [\"MildDemented\", \"VeryMildDemented\", \"NonDemented\"]  # Eliminado \"ModerateDemented\"\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Graficar la matriz de confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels_classes, yticklabels=labels_classes)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c1pRNDasYwcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertir las etiquetas en formato binario (one-vs-all) para 3 clases\n",
        "y_true_bin = label_binarize(y_true, classes=[0, 1, 2])  # Solo 3 clases ahora\n",
        "y_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)  # Obtiene los logits (valores antes de softmax)\n",
        "        y_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "y_scores = np.array(y_scores)\n",
        "\n",
        "# Graficar la curva ROC para cada clase\n",
        "plt.figure(figsize=(8,6))\n",
        "labels_classes = [\"MildDemented\", \"VeryMildDemented\", \"NonDemented\"]  # Sin \"ModerateDemented\"\n",
        "\n",
        "for i in range(3):  # Ahora solo 3 clases\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Clase {labels_classes[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Curvas ROC por Clase\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EgUvlFI0Yvy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}