{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFUx_QxL51C",
        "outputId": "e00a095d-06fc-4892-a062-df632a4a3e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset descargado y descomprimido en: alzheimers_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Descargar dataset desde Kaggle\n",
        "dataset_path = \"alzheimers_data\"\n",
        "kaggle_dataset = \"yiweilu2033/well-documented-alzheimers-dataset\"\n",
        "\n",
        "# Crear la carpeta si no existe\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path, exist_ok=True)\n",
        "    os.system(f\"kaggle datasets download -d {kaggle_dataset} -p {dataset_path} --unzip\")\n",
        "    print(\"Dataset descargado y descomprimido en:\", dataset_path)\n",
        "else:\n",
        "    print(\"El dataset ya está descargado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Verificar si la ruta del dataset existe\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Error: La ruta del dataset no existe.\")\n",
        "else:\n",
        "    # Diccionario para almacenar el número de imágenes por clase\n",
        "    image_counts = {}\n",
        "\n",
        "    # Recorrer las carpetas dentro del dataset\n",
        "    for class_name in sorted(os.listdir(dataset_path)):  # Ordenar alfabéticamente\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "\n",
        "        if os.path.isdir(class_path):  # Verificar que es una carpeta\n",
        "            num_images = 0\n",
        "\n",
        "            # Recorrer todos los archivos en la carpeta y sus subcarpetas\n",
        "            for root, _, files in os.walk(class_path):\n",
        "                num_images += sum(1 for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')))\n",
        "\n",
        "            image_counts[class_name] = num_images\n",
        "\n",
        "    # Mostrar el número de imágenes por clase\n",
        "    for class_name, count in image_counts.items():\n",
        "        print(f\" {class_name}: {count} imágenes\")\n",
        "\n",
        "    # Mostrar total de imágenes en todo el dataset\n",
        "    total_images = sum(image_counts.values())\n",
        "    print(f\"\\n Total de imágenes en el dataset: {total_images}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ26iHW0MJz5",
        "outputId": "c6fe77ba-819b-4dda-dd3c-227a6e6e4862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MildDemented: 5184 imágenes\n",
            " ModerateDemented: 376 imágenes\n",
            " NonDemented (2): 63560 imágenes\n",
            " VeryMildDemented: 13796 imágenes\n",
            "\n",
            " Total de imágenes en el dataset: 82916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Transformaciones para preprocesar imágenes (según el modelo del paper)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Ajuste inicial según la entrada del modelo\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Aumentación de datos\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalización en imágenes RGB\n",
        "])\n",
        "\n",
        "# Cargar dataset\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# División en 80%-10%-10%\n",
        "train_size = int(0.80 * len(dataset))\n",
        "val_size = int(0.10 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size  # Ajuste para que sumen correctamente\n",
        "\n",
        "torch.manual_seed(42)  # Reproducibilidad\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Crear dataloaders\n",
        "batch_size = 128  # Reducido para evitar problemas de memoria\n",
        "num_workers = 4\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnw17V_uOUdX",
        "outputId": "12306eb5-d198-465f-f3e6-58d058c1860a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 66332, Validation: 8291, Test: 8293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AlzheimerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=1):  # El paper usa salida binaria\n",
        "        super(AlzheimerCNN, self).__init__()\n",
        "\n",
        "        # Bloque 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bloque 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bloque 3\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bloque 4\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bloque 5\n",
        "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calcular tamaño de la salida antes de la capa fully connected\n",
        "        self.flattened_size = self._get_flattened_size()\n",
        "\n",
        "        # Capas totalmente conectadas\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 121)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(121, num_classes)\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        \"\"\"Calcula automáticamente el tamaño de la salida antes de la capa fully connected.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, 224, 224)  # Imagen de prueba con tamaño 224x224\n",
        "            x = self.pool1(F.relu(self.conv1(dummy_input)))\n",
        "            x = self.pool2(F.relu(self.conv2(x)))\n",
        "            x = self.pool3(F.relu(self.conv3(x)))\n",
        "            x = self.pool4(F.relu(self.conv4(x)))\n",
        "            x = self.pool5(F.relu(self.conv5(x)))\n",
        "            return x.view(1, -1).size(1)  # Obtener tamaño correcto para la capa FC\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        x = self.pool4(F.relu(self.conv4(x)))\n",
        "        x = self.pool5(F.relu(self.conv5(x)))\n",
        "\n",
        "        x = torch.flatten(x, 1)  # Aplanar para fully connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Inicializar modelo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AlzheimerCNN(num_classes=4).to(device)\n",
        "\n",
        "# Comprobar arquitectura\n",
        "print(model)\n",
        "\n",
        "# Prueba con un batch de imágenes de 224x224\n",
        "dummy_images = torch.randn(16, 3, 224, 224).to(device)\n",
        "outputs = model(dummy_images)\n",
        "\n",
        "print(\"Output shape:\", outputs.shape)  # Debería ser [16, 4] porque hay 4 clases\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq6dkMN2PQsW",
        "outputId": "a24768fa-6567-4871-cdd6-ce9650147e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlzheimerCNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1568, out_features=121, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=121, out_features=4, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([16, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Configuración de hiperparámetros\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Función de pérdida\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizador Adam\n",
        "num_epochs = 25  # Número de épocas\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Modo de entrenamiento\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Reiniciar gradientes\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Cálculo de pérdida\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Actualizar pesos\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / total\n",
        "\n",
        "    # Evaluación en validación\n",
        "    model.eval()\n",
        "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "          f\"Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
        "          f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTiempo total de entrenamiento: {training_time:.2f} segundos\")\n",
        "\n",
        "# Guardar modelo entrenado\n",
        "torch.save(model.state_dict(), \"alzheimers_model.pth\")\n",
        "print(\"Modelo guardado como 'alzheimers_model.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6pq2c-IPbhp",
        "outputId": "2a269384-ad92-4c0a-f7e2-30adae372d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25] - Loss: 0.6299, Accuracy: 76.38%, Val Loss: 0.5847, Val Accuracy: 76.50%\n",
            "Epoch [2/25] - Loss: 0.5312, Accuracy: 77.17%, Val Loss: 0.4707, Val Accuracy: 77.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluación en el conjunto de test\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Matriz de confusión\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Visualización\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=dataset.classes))\n"
      ],
      "metadata": {
        "id": "J3hIWLW8P2m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarización de etiquetas para ROC (One-vs-Rest)\n",
        "num_classes = len(dataset.classes)\n",
        "y_true_bin = label_binarize(all_labels, classes=range(num_classes))\n",
        "y_pred_prob = torch.softmax(torch.tensor(all_preds), dim=1).cpu().numpy()  # Convertimos las predicciones a probabilidades\n",
        "\n",
        "# Curva ROC para cada clase\n",
        "plt.figure(figsize=(8,6))\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"Class {dataset.classes[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Línea base\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b7thmy6DP47B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}